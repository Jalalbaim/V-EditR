# V-EditR

**V-EditR** â€” A reasoning-first image editor powered by Visionâ€“Language Models for intelligent, context-aware image manipulation.

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange.svg)](https://pytorch.org/)

</div>

## ğŸ¯ Overview

V-EditR is an advanced image editing pipeline that understands natural language instructions and applies precise edits based on semantic reasoning. Unlike traditional image editors that rely purely on style cues, V-EditR analyzes scene context, object relationships, and spatial information before making modifications.

### Why V-EditR?

Modern image editing tools often fail on requests that require understanding relations, counts, or context. For example:

- âŒ "Add two red cars next to the blue truck." â†’ Adds cars in wrong locations
- âŒ "Make the person holding the phone wear a black jacket." â†’ Modifies wrong person
- âŒ "Remove the chair behind the table." â†’ Removes wrong object

V-EditR solves these problems by:

âœ… **Understanding Context** â€” Interprets free-form instructions with semantic reasoning  
âœ… **Spatial Reasoning** â€” Handles relations like "next to", "behind", "holding"  
âœ… **Object Grounding** â€” Precisely locates objects using GroundingDINO + SAM  
âœ… **Smart Editing** â€” Applies targeted modifications without disturbing unrelated content

## ğŸ—ï¸ Architecture

V-EditR operates through a multi-stage pipeline:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text Instructionâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Plan Generator â”‚ â”€â”€â”€â–º â”‚ Operation(s) â”‚
â”‚  (LLM Parser)   â”‚      â”‚ + Targets    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Object Grounding    â”‚
                    â”‚  â€¢ GroundingDINO     â”‚
                    â”‚  â€¢ SAM (Segment)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Edit Application   â”‚
                    â”‚  â€¢ InstructPix2Pix   â”‚
                    â”‚  â€¢ Add-It/ControlNet â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Edited Image       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Getting Started

### Prerequisites

- **Python**: 3.8 or higher
- **CUDA**: 11.8+ (recommended for GPU acceleration)
- **RAM**: 8GB minimum, 16GB+ recommended
- **GPU**: NVIDIA GPU with 8GB+ VRAM (for optimal performance)

### Installation

1. **Clone the repository**

   ```bash
   git clone https://github.com/Jalalbaim/V-EditR.git
   cd V-EditR
   ```

2. **Create a virtual environment**

3. **Install dependencies**

   ```bash
   # For CUDA 12.1
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

   # For CPU only
   pip install torch torchvision torchaudio

   # Install other requirements
   pip install requirements.txt
   ```

4. **Download model weights**

   Create a `weights/` directory and download the following:

   - **GroundingDINO**: [groundingdino_swint_ogc.pth](https://github.com/IDEA-Research/GroundingDINO/releases)
   - **SAM**: [sam_vit_h_4b8939.pth](https://github.com/facebookresearch/segment-anything#model-checkpoints)

5. **Configure paths**

   Edit `configs/grounding.yaml` to update checkpoint paths:

   ```yaml
   grounding:
     dino:
       ckpt: "path/to/your/groundingdino_swint_ogc.pth"
     sam:
       ckpt: "path/to/your/sam_vit_h_4b8939.pth"
   ```

## ğŸ“– Usage

### Basic Command

```powershell
python scripts/run_edit.py --image assets/sample.jpeg --instruction "add a red car next to the truck" --tag my_edit
```

### Parameters

- `--image`: Path to input image (required)
- `--instruction`: Text instruction describing the edit (required)
- `--tag`: Custom tag for the output folder (default: "phase4")

### Examples

```powershell
# Add object
python scripts/run_edit.py --image assets/sample.jpeg --instruction "add two blue cars on the road"

# Remove object
python scripts/run_edit.py --image assets/sample.jpeg --instruction "remove the truck"

# Modify attributes
python scripts/run_edit.py --image assets/sample.jpeg --instruction "make the truck red"

# Complex relations
python scripts/run_edit.py --image assets/sample.jpeg --instruction "add a person next to the car holding an umbrella"
```

### Output Structure

Results are saved in `runs/TIMESTAMP_TAG/`:

```
runs/
â””â”€â”€ 20251113_143752_my_edit/
    â”œâ”€â”€ run_summary.json        # Execution metadata
    â””â”€â”€ artifacts/
        â”œâ”€â”€ input.jpg           # Original image
        â”œâ”€â”€ edited.jpg          # Final result
        â”œâ”€â”€ plan.json           # Generated action plan
        â”œâ”€â”€ grounding.json      # Object detection results
        â”œâ”€â”€ boxes_*.jpg         # Bounding box visualizations
        â”œâ”€â”€ masks_*.jpg         # Segmentation masks
        â”œâ”€â”€ validator.json      # Validation report
        â””â”€â”€ verifier.json       # Verification results
```

### Project Structure

```
V-EditR/
â”œâ”€â”€ configs/              # Configuration files
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ editors/          # Image editing modules
â”‚   â”œâ”€â”€ grounding/        # Object detection & segmentation
â”‚   â”œâ”€â”€ planners/         # Instruction parsing
â”‚   â”œâ”€â”€ validators/       # Output validation
â”‚   â””â”€â”€ verifiers/        # Result verification
â”œâ”€â”€ scripts/              # Execution scripts
â”œâ”€â”€ tests/                # Unit tests
â”œâ”€â”€ weights/              # Model checkpoints
â””â”€â”€ runs/                 # Output directory
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ™ Acknowledgments

- [GroundingDINO](https://github.com/IDEA-Research/GroundingDINO) - Open-set object detection
- [Segment Anything (SAM)](https://github.com/facebookresearch/segment-anything) - Instance segmentation
- [InstructPix2Pix](https://www.timothybrooks.com/instruct-pix2pix/) - Instruction-based editing
- [Stable Diffusion](https://stability.ai/) - Generative models

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub.

**Author**: Jalal Baim  
**Repository**: [https://github.com/Jalalbaim/V-EditR](https://github.com/Jalalbaim/V-EditR) â€” a reasoning-first image editor powered by a Visionâ€“Language Model.

---
